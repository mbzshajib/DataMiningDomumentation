%\documentclass[a4paper,12pt]{book}
%\usepackage{pgfplots}
%\usepackage[justification=centering]{caption}
%\pgfplotsset{compat=newest}
%\begin{document}
\chapter{Experimental Results}
\lhead{Chapter 4. \emph{Experimental Results}}
In this chapter we have shown our experimental results achieved by our proposed approach. Based on several performance metric we have tried to show our algorithms' efficiency and performance. We have taken different scale/parameter to evaluate our algorithm


\section{Experimental Settings}
We have performed a number of simulations in our experiment on both synthetic database and real-world database. The data are taken from dataset repository ~\cite{dataset}. Our experiment shows that \emph{US-tree} (Uncertain Stream tree) is very much compact. This tree construction technique can make the items share one single node. This compactness of \emph{US-tree} surprisingly helps the mining, \emph{USFP-growth} (Uncertain Stream Frequent Pattern growth) process to gain a lot in run-time and memory. Moreover, our proposed pattern tree can be used to find max pattern and closed pattern. Performance tests from our experiment show that \emph{US-tree} tree construction technique and \emph{USFP-growth} mining algorithm can run on any uncertain stream database with any support threshold, window size, and batch size. Our experimental result shows that these techniques are much faster and scalable frequent pattern mining technique. As we have proposed a new approach for finding frequent patterns over uncertain data we have compared performance with itself for comparing correctness of our approach. Then we have compared with all well known existing approaches for finding frequent item-sets over the uncertain database. \emph{SUF-growth} is one of them. We have tried to compare in all aspects to prove our approach's correctness, run-time efficiency, and memory efficiency.
\input{table/table_configuration}
All program for the simulating experimental result are written \emph{Java} programming language that run on \emph{Java Runtime Environment (JRE) - 1.7.0.79}. All program was run on a computer having \emph{3.4 GHz Intel(R) Core(TM) i-7} processor and \emph{8 GB RAM} with \emph{Windows-7, 64-bit, service pack-1} operating system installed in it (table-[\ref{table:experiment_configuration}]). For management of the experimental project, Gradle was used as the build tool. Results shown in this chapter are based on the average of multiple runs for every case. \emph{US-tree} was constructed with the chronological order of database items. All the running time includes \emph{CPU}, \emph{I/O}.\\
\input{result/g_normal_distribution}
we got the synthetic and real-life datasets from the frequent item-set mining repository ~\cite{dataset}, those were collected for certain databases. Then we have used our own probabilistic tool and technique to generate the existential probability of each item of the each transaction of the database. Real life data set actually follows Gaussian distribution that is normal distribution [\ref{result:normal_distribution}]. It actually says that in real world extreme cases are minimum and average cases are maximum. From the figure [\ref{result:normal_distribution}] we can see that in the middle the pick value is highest so we can say count item probability at \emph{.5} is maximum. So we used this technique to generate and introduce existential probability to each item in a transaction. We have used \emph{Java pseudo random} generate the existential probability for each item of all the transaction of the database. By assigning these probability value to each item, we have generated the uncertain database for both real life database and synthetic database found from dataset repository ~\cite{dataset}. However, one can give existential probability by any distribution according to need.


\section{Performance Metrics}
We have considered several metrics as parameters for evaluating our proposed algorithm. We have set several properties for this evaluation from experimental result. As we have worked on the data set that comes like the stream so we have set parameters for both the frequent item set mining from total data set and per window. The parameters and properties are given below:

\begin{itemize}
    \item {Correctness}
    \begin{itemize}
        \item batch size vs running time.
        \item window size vs running time.
        \item transactions in a window vs false positive count.
    \end{itemize}
    \item {Comparison with existing approaches}
    \begin{itemize}
        \item Tree construction time per window and total database vs minimum support.
        \item Mining time per window and total database vs minimum support.
        \item Total time to complete per window and total database vs minimum support.
        \item Total tree node in tree per window vs minimum support.
        \item Total memory needed by mining process vs minimum support.
    \end{itemize}
\end{itemize}
\section{Experimental Environment}
For our experimental evaluation, we used both real life database and synthetic database from database repository ~\cite{dataset}. Table [\ref{table:dataset}] shows the database type and properties.
        \begin{table}[h]
        \centering
        \begin{tabular}{|c|c|c|c|c|}
        \hline 
        Name        &    Type    &    Density    &    Total Transaction     &    Distinct Items    \\ \hline \hline
        mushroom    &    real    &    dense    &    8124    &    120                            \\ \hline
        kosarak        &    real    &    sparse    &    990002    &    41270                        \\ \hline
        pumsb star    &    real    &    sparse    &    49046    &    2088                        \\ \hline
        chess        &    real    &    dense    &    3196    &    75                            \\ \hline
        T10I4D100K    &    synthetic    &    sparse    &    100000    &    869                        \\ \hline
            \end{tabular}
        \caption{Dataset from repository ~\cite{dataset}}
        \label{table:dataset}
        \end{table}


\subsection{Real Life Data Set}
To generate uncertain database from certain we have followed normal distribution formula. For this we have used mean value $0.5$ and variance value $0.5 / \pi$
For real life data sets we have used mushroom ~\cite{dataset}, chess ~\cite{dataset} and kosarak ~\cite{dataset}. Mushroom and chess are dense datasets and kosarak is sparse dataset. Mushroom has 8124 transactions with 120 distinct items and chess has 3196 transactions with 75 distinct items and kosarak has 99002 transaction and 41270 distinct item. For probability assignment to each item, we used the normal distribution for getting the existential probability. For giving existential probability to each item of each transaction in the database, we have followed the normal distribution described earlier. Figure \ref{result:g_dataset_mushroom}, figure \ref{result:g_dataset_chess} and \ref{result:g_dataset_kosarak} shows the probability distribution for corresponding mushroom, chess and kosarak.
        \begin{figure}[h]
        \centering
            \input{result/mushroom/g_dataset_mushroom}
        \caption{Probability Distribution for Mushroom ~\cite{dataset} Dataset}
        \label{result:g_dataset_mushroom}
        \end{figure}
        
        \begin{figure}[h]
        \centering
            \input{result/chess/g_dataset_chess}
        \caption{Probability Distribution for Chess ~\cite{dataset} Dataset}
        \label{result:g_dataset_chess}
        \end{figure}
        \begin{figure}[h]
        \centering
            \input{result/kosarak/g_dataset_kosarak}
        \caption{Probability Distribution for Kosarak ~\cite{dataset} Dataset}
        \label{result:g_dataset_kosarak}
        \end{figure}


\subsection{Synthetic Data Set}
For synthetic data sets, we have used T10I4D100K ~\cite{dataset}. It is an IBM generated transaction data set widely used for frequent pattern mining. It is a sparse data set with 100000 transactions and 869 distinct items. For probability assignment to each item, we used the normal distribution for getting the existential probability.
        \begin{figure}[h]
        \centering
            \input{result/t10/g_dataset_t10}
        \caption{Probability Distribution for T40I10D100K ~\cite{dataset} Dataset}
        \label{result:g_dataset_t10}
        \end{figure}
        
        
\clearpage
\section{Comparison and Analysis}
    \input{result/simulation}

\clearpage
\section{Summary}
In summary, our proposed \emph{US-tree}, construction \emph{USFP-growth} mining algorithm is very correct, efficient, the scalable algorithm that works fine in any configuration (minimum support, window size, batch size). For dense dataset (both real and synthetic) this is very efficient. For sparse dataset, this also gives us gain both in memory and in running time. The \emph{U\textsuperscript{cap}} value gives much more benefit to share nodes in the \emph{US-tree}. The compactness of \emph{US-tree} is surprisingly noticeable. Mining compact \emph{US-tree} gives the main surprise in the running time. The \emph{USFP-growth} mining algorithm works nicely without generating no false negatives and a little amount of false positives that can efficiently be removed using the false positive reduction technique.
%
%\end{document}