%\documentclass[a4paper,12pt]{book}
%\usepackage{pgfplots}
%\usepackage[justification=centering]{caption}
%\pgfplotsset{compat=newest}
%\begin{document}
\chapter{Experimental Result and Discussion}
\lhead{Chapter 4. \emph{Experimental Result and Discussion}}
In this chapter we have shown our experimental results achieved by our proposed approach. Based on several performance metric we have tried to show our algorithms' efficiency and performance. We have taken different scale/parameter to evaluate our algorithm


\section{Experimental Settings}
We have performed number of simulations in our experiment on both synthetic database and real world database. The data are taken from data set repository ~\cite{dataset}. Our experiment shows that \emph{US-tree} ( Uncertain Stream tree )is very much compact. This tree construction technique can make the items to share one node. This compactness of \emph{US-tree} surprisingly helps the mining, \emph{USFP-growth} ( Uncertain Stream Frequent Pattern growth ) process to gain a lot in run-time and memory. More over our proposed pattern tree can be used to find max patterns and close patterns. Performance tests from our experiment shows that \emph{US-tree} tree construction technique and \emph{USFP-growth} mining algorithm can run on any uncertain stream database with any support threshold, window size and batch size. Our experimental result shows that these techniques are much more faster and scalable frequent pattern mining technique. As we have proposed a new approach for finding frequent patterns over uncertain data we have compared performance with itself for comparing correctness of our approach. Then we have compared with all well known existing approaches for finding frequent item sets over uncertain database. \emph{SUF-growth} is one of them. We have tried to compare in all aspects to prove our approach's correctness, run-time efficiency and memory efficiency.
\input{table/table_configuration}
All program for the simulating experimental result are written \emph{Java} programming language that run on \emph{Java Run time Environment (JRE) - 1.7.0.79}. All program was run on a computer having \emph{3.4 GHz Intel(R) Core(TM) i-7} processor and \emph{8 GB RAM} with \emph{Windows-7, 64-bit, service pack-1} operating system installed in it (Table-[\ref{table:experiment_configuration}]). For management of the experimental project Gradle was used as build tool. Results shown in this chapter are based on average of multiple run for every case. \emph{US-tree} was constructed with chronological order of database items. All the running time includes \emph{CPU}, \emph{I/O}.\\
\input{result/g_normal_distribution}
we got the synthetic and real life datasets from the frequent itemset mining repository ~\cite{dataset}, those were collected for certain databases. Then we have used our own probabilistic tool and technique to generate existential probability of each items of the each transaction of database. Real life data set actually follows Gaussian distribution that is normal distribution [\ref{result:normal_distribution}]. It actually says that in real world extreme cases are minimum and average case are maximum. From the figure [\ref{result:normal_distribution}] we can see that in the middle the pick value is highest so we can say count item probability at \emph{.5} is maximum. So we used this technique to generate and introduce existential probability to each items in a transaction. We have used \emph{Java pseudo random} generate existential probability for each item of all the transaction of database. By assigning these probability value to each items we have generate uncertain database for both real life database and synthetic database found from dataset repository ~\cite{dataset}. However one can give existential probability by any distribution according to need.


\section{Performance Metrics}
We have consider several metrics as parameters for evaluating our proposed algorithm. We have set several property for this evaluation from experimental result. As we have worked on data set that comes like stream so we have set parameters for both the frequent item set mining from total data set and per window. The parameters and properties are given below:

\begin{itemize}
	\item {Correctness}
	\begin{itemize}
		\item batch size vs running time.
		\item window size vs running time.
		\item transactions in a window vs false positive count.
	\end{itemize}
	\item {Comparison with existing approaches}
	\begin{itemize}
		\item Tree construction time per window and total database vs minimum support.
		\item Mining time per window and total database vs minimum support.
		\item Total time to complete per window and total database vs minimum support.
		\item Total tree node in tree per window vs minimum support.
		\item Total memory needed by mining process vs minimum support.
	\end{itemize}
\end{itemize}
\section{Experimental Environment}
For our experimental evaluation we used both real life database and synthetic database from database repository ~\cite{dataset}. Table [\ref{table:dataset}] shows the data base type and properties.
		\begin{table}[h]
		\centering
		\begin{tabular}{|c|c|c|c|c|}
		\hline 
		Name		&	Type	&	Density	&	Total Transaction 	&	Distinct Items	\\ \hline \hline
		mushroom	&	real	&	dense	&	8124	&	120							\\ \hline
		kosarak		&	real	&	sparse	&	990002	&	41270						\\ \hline
		pumsb star	&	real	&	sparse	&	49046	&	2088						\\ \hline
		chess		&	real	&	dense	&	3196	&	75							\\ \hline
		T10I4D100K	&	synthetic	&	sparse	&	100000	&	869						\\ \hline
			\end{tabular}
		\caption{Dataset from repository ~\cite{dataset}}
		\label{table:dataset}
		\end{table}


\subsection{Real Life Data Set}
For real life data sets we have used mushroom ~\cite{dataset} and chess ~\cite{dataset}. Mushroom, chess and pumsb star are dense datasets. Mushroom has 8124 transactions with 120 distinct items and chess has 3196 transactions with 75 distinct items. For probability assignment to each items we used normal distribution for getting existential probability. For giving existential probability to each item of each transaction in database we have followed the natural distribution described earlier. Figure \ref{result:g_dataset_mushroom}, figure \ref{result:g_dataset_chess} shows the probability distribution for corresponding mushroom and chess.
		\begin{figure}[h]
		\centering
			\input{result/mushroom/g_dataset_mushroom}
		\caption{Probability Distribution for Mushroom ~\cite{dataset} Dataset}
		\label{result:g_dataset_mushroom}
		\end{figure}
		
		\begin{figure}[h]
		\centering
			\input{result/chess/g_dataset_chess}
		\caption{Probability Distribution for Chess ~\cite{dataset} Dataset}
		\label{result:g_dataset_chess}
		\end{figure}

\subsection{Synthetic Data Set}
For synthetic data sets we have used T10I4D100K ~\cite{dataset}. It is an IBM generated transaction data set widely used for frequent pattern mining. It is a sparse data set with 100000 transactions and 869 distinct items. For probability assignment to each items we used normal distribution for getting existential probability.
		\begin{figure}[h]
		\centering
			\input{result/t10/g_dataset_t10}
		\caption{Probability Distribution for T40I10D100K ~\cite{dataset} Dataset}
		\label{result:g_dataset_t10}
		\end{figure}
		
		
\clearpage
\section{Comparison and Analysis}
	\input{result/simulation}

\clearpage
\section{Summary}
In summary, our proposed \emph{US-tree}, construction \emph{USFP-growth} mining algorithm is very correct, efficient, scalable algorithm that works fine in any configuration (minimum support, window size, batch size). For dense dataset (both real and synthetic) this is very efficient. For sparse dataset this also gives us gain both in memory and in running time. The \emph{U\textsuperscript{cap}} value gives much more benefit to share nodes in the \emph{US-tree}. The compactness of \emph{US-tree} is surprisingly noticeable. Mining compact \emph{US-tree} gives the main surprise in the running time. The \emph{USFP-growth} mining algorithm works nicely without generating no false negatives and a little amount of false positives that can efficiently be removed using the false positive reduction technique.
%
%\end{document}